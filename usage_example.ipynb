{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from dataset import NewsDataset\n",
    "from model import DistilBertForSequenceClassification\n",
    "\n",
    "from smooth_gradient import SmoothGradient\n",
    "from integrated_gradient import IntegratedGradient\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import DistilBertConfig, DistilBertTokenizer\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitaliy/anaconda3/envs/sqrt/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.22.2.post1 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = DistilBertConfig()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "distilbert = DistilBertForSequenceClassification(config, num_labels=93)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "path = '/media/vitaliy/9C690A1791D68B8B/after/learningfolder/distilbert_medium_titles/distilbert.pth'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    distilbert.load_state_dict(\n",
    "        torch.load(path)\n",
    "    )\n",
    "else:\n",
    "    distilbert.load_state_dict(\n",
    "        torch.load(path, map_location=torch.device('cpu'))\n",
    "    )\n",
    "    \n",
    "with open('../label_encoder.sklrn', 'rb') as f:\n",
    "    le = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = [\n",
    "    [\"Interpretation of HuggingFase's modelÂ decision\"], \n",
    "    [\"Transformer-based models have taken a leading role\"\n",
    "     \"in NLP today.\"]\n",
    "]\n",
    "\n",
    "test_dataset = NewsDataset(\n",
    "    data_list=test_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.max_position_embeddings, \n",
    "    one_example=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/vitaliy/vitaliy/workfolder/repos/Interpret/integrated_gradient.py(96)_integrate_gradients()\n",
      "-> embeddings_list.reverse()\n",
      "(Pdb) type(grads)\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# smooth_grad = SmoothGradient(distilbert, criterion)\n",
    "# instances = smooth_grad.saliency_interpret(test_dataset)\n",
    "integrated_grads = IntegratedGradient(distilbert, criterion)\n",
    "instances = integrated_grads.saliency_interpret(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coloder_string = smooth_grad.colorize(instances['instance_1'])\n",
    "display(HTML(coloder_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = instances['instance_1']['label_input_1']\n",
    "print(f\"Converted label #{label}: {le.inverse_transform([label])[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqrt",
   "language": "python",
   "name": "sqrt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
